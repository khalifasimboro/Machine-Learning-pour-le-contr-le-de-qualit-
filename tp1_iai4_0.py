# -*- coding: utf-8 -*-
"""TP1_IAI4_0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zI6OZ778oW0ugXJ9G2wdqjrygwuKbaOO

**TP 1 : Machine Learning pour le contr√¥le de qualit√©**

**1. Objectif**

L‚Äôobjectif de ce TP est de d√©velopper un mod√®le de classification pour le contr√¥le qualit√© des puces
semi-conductrices.
"""

!pip install imblearn

""" **2.Prise en main de la dataset**"""

#Import librairies de manipulation et de visualisation

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#Importation de la dataset

file_path = "/content/uci-secom.csv"
data = pd.read_csv(file_path)

#Afficher les dimensions du dataframe

print(data.shape)

#Afficher les 10 premi√®res lignes

data.head(10)

"""Interpr√©tation :
- Valeurs manquantes
- Variances √©l√©v√©s par rapport aux valeurs des colonnes
- Les colonnes n√©cessitent une standarisation ou une normalisation
- Les √©tiquettes de colonnes sont de cat√©gories diff√©rentes (phase/aux autres colonnes)
- Le capteur No 5 affiche la meme valeur plusieurs fois en premi√®re constation
- Le capteur No 583 admet des diff√©rence de valeur d'ordre de 10x2
"""

#Utiliser la m√©thode info() pour afficher les d√©tails des colonnes

data.info()

"""Commentaire üá∞
- 590 colonnes de type float (nombre √† virgule)
- 1 colonne de type entier(probablement la target)
- 2 colonne de type objet( √† encoder surement)
"""

#Utiliser la m√©thode describe() pour afficher les quelques statistiques sur les donn√©es

data.describe()

"""Commentaires
- La variance du feature N05 est nulles
- Les variances sont tr√®s diff√©rentes donc standarisation des colonnes
-colonnes √† faible variance donc suppression imminente car n'impacte pas la target
"""

#Detection des valeurs manquantes

data.isnull().sum()

"""Commentaire :  colonnes ayant des valeurs manquantes tr√®s √©lev√©es"""

# V√©rification des doublons

data.duplicated().sum()

"""Commentaire : Aucune donn√©e rep√©t√©e"""

# V√©rification de l'√©quilibrage des classes

data['Pass/Fail'].value_counts()

"""Commentaire : les donn√©es sont fortement d√©s√©quilibr√©es

**3.Analyse et pr√©traitement des donn√©es**
"""

#V√©rifier les types de donn√©es des diff√©rentes colonnes

"""Diff√©rentes op√©rations √† faire:
- Encoder la colonne Phase avec la m√©thode labelencoder pour √©viter d'augementer
- Equilibrage des donn√©es
- supprimer la colonne time car pas importante
- Standarisation de toutes les colonnes
- Suppression des colonnes √† faibles variances
- normalisation des colonnes
- Suppression des colonnes avec des valeurs manquantes elev√©es


"""

#encoder la colonne Phase avec le levelencoder
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
data['Phase'] = encoder.fit_transform(data['Phase'])

data.head(10)

#suppession de la colonne time

data.drop('Time', axis=1, inplace=True)
data.head(10)

#suppression des colonnes √† faible variance
col = data[['Phase','Pass/Fail']] # Changed to list of columns
data.drop(col, axis=1, inplace=True)

# Calculer l'√©cart type de chaque colonne
ecarts_types = data.std()

# D√©finir le seuil d'√©cart type
seuil_ecart_type = 10  # Par exemple

# S√©lectionner les colonnes √† conserver
colonnes_a_conserver = ecarts_types[ecarts_types > seuil_ecart_type].index

# Cr√©er un nouveau DataFrame avec les colonnes s√©lectionn√©es
new_data = data[colonnes_a_conserver]

final_df = pd.concat([new_data, col], axis=1) # Concatenate back

final_df.head(10)

#Detection des valeurs manquantes

missing_values = final_df.isnull().sum()
print(missing_values)

from typing_extensions import final
#suppression des colonnes avec plusieurs valeurs manquantes

# S√©lectionner les colonnes √† supprimer
columns_to_drop = missing_values[missing_values > 1].index

# Supprimer les colonnes du DataFrame
final_df = final_df.drop(columns=columns_to_drop)

final_df.head(10)

final_df.shape

#Remplacer les valeurs manquantes par leur le plus proche voisin

from sklearn.impute import KNNImputer

# Cr√©er une instance de KNNImputer
imputer = KNNImputer(n_neighbors=3)  # Vous pouvez ajuster le nombre de voisins si n√©cessaire

# Ajuster l'imputer aux donn√©es et les transformer
data_imputed = imputer.fit_transform(final_df)

# Cr√©er un nouveau DataFrame avec les valeurs imput√©es
final_df_imputed = pd.DataFrame(data_imputed, columns=final_df.columns)

final_df_imputed.head(10)

final_df_imputed.isnull().sum()

#Equilibrage des classes

from imblearn.over_sampling import SMOTE

# Instancier SMOTE
smote = SMOTE(random_state=42)

# Diviser les donn√©es
X = final_df_imputed.drop('Pass/Fail', axis=1)
y = final_df_imputed['Pass/Fail']

# Appliquer SMOTE
X_resampled, y_resampled = smote.fit_resample(X, y)

# Cr√©er un nouveau DataFrame
balanced_df = pd.DataFrame(X_resampled, columns=X.columns)
balanced_df['Pass/Fail'] = y_resampled

balanced_df.head(10)
balanced_df['Pass/Fail'].value_counts()

#standarisation

import pandas as pd
from sklearn.preprocessing import StandardScaler
import joblib

# Instancier le StandardScaler
scaler = StandardScaler()

# Ajuster le scaler aux donn√©es et les transformer
X_scaled = scaler.fit_transform(balanced_df.drop('Pass/Fail', axis=1))

# Cr√©er un nouveau DataFrame avec les donn√©es standardis√©es
scaled_df = pd.DataFrame(X_scaled, columns=balanced_df.drop('Pass/Fail', axis=1).columns)
scaled_df['Pass/Fail'] = balanced_df['Pass/Fail']

# Archiver le mod√®le avec Joblib
joblib.dump(scaler, 'scaler_model.pkl')

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import joblib

# Instancier le MinMaxScaler
normalizer = MinMaxScaler()

# Ajuster le normalisateur aux donn√©es et les transformer
X_normalized = normalizer.fit_transform(scaled_df.drop('Pass/Fail', axis=1))

# Cr√©er un nouveau DataFrame avec les donn√©es normalis√©es
normalized_df = pd.DataFrame(X_normalized, columns=scaled_df.drop('Pass/Fail', axis=1).columns)
normalized_df['Pass/Fail'] = scaled_df['Pass/Fail']

# Archiver le mod√®le avec Joblib
joblib.dump(normalizer, 'normalizer_model.pkl')

import pandas as pd
from sklearn.decomposition import PCA
import joblib

# Instancier le PCA
pca = PCA(n_components=0.95)

# Ajuster le PCA aux donn√©es et les transformer
X_pca = pca.fit_transform(normalized_df.drop('Pass/Fail', axis=1))

# Cr√©er un nouveau DataFrame avec les composantes principales
pca_df = pd.DataFrame(X_pca)
pca_df['Pass/Fail'] = normalized_df['Pass/Fail']

# Archiver le mod√®le avec Joblib
joblib.dump(pca, 'pca_model.pkl')

pca_df.head()

# Afficher les boxplots pour toutes les colonnes num√©riques

plt.figure(figsize=(12, 6))
sns.boxplot(data=pca_df)
plt.xticks(rotation=45)  # Rotation des noms de colonnes si n√©cessaire
plt.title("D√©tection des valeurs aberrantes")
plt.show()

# D√©finition d'une fonction pour d√©tecter et supprimer les outliers
def remove_outliers(df):
    Q1 = df.quantile(0.25)  # 1er quartile
    Q3 = df.quantile(0.75)  # 3e quartile
    IQR = Q3 - Q1  # Interquartile Range

    # D√©finir les bornes pour d√©tecter les outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Filtrer les donn√©es sans outliers
    df_cleaned = df[~((df < lower_bound) | (df > upper_bound)).any(axis=1)]

    return df_cleaned

# Supprimer les valeurs aberrantes
pca_df_cleaned = remove_outliers(pca_df)

print(f"Taille avant nettoyage : {pca_df.shape}")
print(f"Taille apr√®s nettoyage : {pca_df_cleaned.shape}")

#Visualisation apr√®s nettoyage

plt.figure(figsize=(12, 6))
sns.boxplot(data=pca_df_cleaned)
plt.xticks(rotation=45)
plt.title("Donn√©es apr√®s suppression des outliers")
plt.show()

#Visualiser la distribution des donn√©es

# Histogramme de toutes les colonnes num√©riques
pca_df_cleaned.hist(figsize=(12, 8), bins=30, edgecolor='black')
plt.suptitle("Distribution des variables")
plt.show()

# Courbes de densit√© (plus lisible que l'histogramme)
pca_df_cleaned.plot(kind='kde', figsize=(12, 6), title="Densit√© des variables")
plt.show()

# Calculer la matrice de corr√©lation
correlation_matrix = pca_df_cleaned.corr()

# Cr√©er la heatmap
plt.figure(figsize=(10, 8))  # Ajuster la taille de la figure si n√©cessaire
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matrice de corr√©lation')
plt.show()

"""**4.Lancement de l'apprentissage**"""

# Diviser les donn√©es en ensembles d'entra√Ænement et de test
from sklearn.model_selection import train_test_split

X = pca_df_cleaned.drop('Pass/Fail', axis=1)
y = pca_df_cleaned['Pass/Fail']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # 70% pour l'entra√Ænement, 20% pour le test

#entrainement des mod√®les

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt

# Instancier les mod√®les avec des hyperparam√®tres
decision_tree = DecisionTreeClassifier(max_depth = 12, min_samples_split=8)
random_forest = RandomForestClassifier(n_estimators=200, max_depth=12)
# Entra√Æner les mod√®les
decision_tree.fit(X_train, y_train)
random_forest.fit(X_train, y_train)

# Faire des pr√©dictions
y_pred_dt = decision_tree.predict(X_test)
y_pred_rf = random_forest.predict(X_test)

# √âvaluer les mod√®les

# ... (Code pour l'instanciation, l'entra√Ænement et la pr√©diction des mod√®les) ...

# √âvaluer les mod√®les
for model_name, y_pred in [("Arbre de d√©cision", y_pred_dt), ("For√™t al√©atoire", y_pred_rf)]:
    print(f"\n√âvaluation de {model_name}:")

    # Matrice de confusion
    cm = confusion_matrix(y_test, y_pred)
    # Affichage sous forme de heatmap
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', linewidths=1, linecolor='black')
    plt.xlabel('Pr√©dictions')
    plt.ylabel('R√©el')
    plt.title(f'Matrice de Confusion de {model_name} ')
    plt.show()


    # Classification Report
    print("Classification Report:\n", classification_report(y_test, y_pred))

 # ... (Code pour afficher la courbe ROC) ...

    # Courbe ROC et AUC
    fpr, tpr, thresholds = roc_curve(y_test, y_pred)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f}) - {model_name}')

"""**6.Validation simple et crois√©e**"""

#Validation simple
from sklearn.metrics import accuracy_score

# Evaluate with Accuracy
accuracy = accuracy_score(y_test, y_pred_rf)
print(f"Simple Validation Accuracy: {accuracy}")

#Validation et crois√©e

from sklearn.model_selection import cross_validate
score = cross_validate(random_forest, X, y, cv=5, scoring=['accuracy'])

print(f"Cross-Validation Scores: {score}")
print(f"Average Cross-Validation Accuracy: {score['test_accuracy'].mean()}") # Access the 'test_accuracy' key to get the accuracy scores and then calculate the mean

"""**7.Optimisation des hyperparam√®tres**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, ShuffleSplit

# D√©finir la validation crois√©e (6 splits, 30% des donn√©es en test)
cv = ShuffleSplit(n_splits=6, test_size=0.3, random_state=0)

# D√©finir la grille de recherche des hyperparam√®tres
param_grid = {
    "n_estimators": [50, 100, 200],  # Nombre d'arbres dans la for√™t
    "max_depth": np.arange(1, 20),   # Profondeur maximale de l‚Äôarbre
}

# Initialiser le mod√®le de Random Forest
rf_model = RandomForestClassifier(random_state=0)

# Lancer la recherche d'hyperparam√®tres avec validation crois√©e
grid_search = GridSearchCV(rf_model, param_grid, cv=cv, scoring="accuracy", n_jobs=-1)
grid_search.fit(X, y)

# Acc√©der aux meilleurs hyperparam√®tres
best_params = grid_search.best_params_
best_params

# Acc√©der au meilleur score
best_score = grid_search.best_score_
best_score

"""**5.Sauvegarde du mod√®le le plus performant**"""

# Instancier le mod√®le avec les meilleurs hyperparam√®tres
random_forest = RandomForestClassifier(n_estimators=200, max_depth=19)
# Entra√Æner les mod√®les
random_forest.fit(X_train, y_train)

#Sauvegarde du mod√®le
import joblib

# ... (Code pour l'entra√Ænement du mod√®le de for√™t al√©atoire) ...

# Sauvegarder le mod√®le
joblib.dump(random_forest, 'random_forest_model.pkl')

# tester la pr√©diction d'un jeu de donn√©es

import pandas as pd
from joblib import load

loaded_model = load('random_forest_model.pkl')

# Use .iloc to select the first row as a DataFrame, then access its values and reshape
prediction = loaded_model.predict(X_test.iloc[[1]].values.reshape(1, -1))
print("Prediction:", prediction)

"""** 8.Archivage avec WandB**"""

!pip install wandb

import wandb
from wandb.integration.keras import WandbMetricsLogger
import joblib

wandb.login()

# cr√©ation du projet, avec un nom de run
wandb.init(project="ECC-pre", name="arbre de d√©cision - jeu 2", reinit=True)

# r√©sultat √† archiver
report = classification_report(y_test, y_pred_rf,  output_dict=True)

#config √† archiver
config = {
    "max_depth": 10,
}

wandb.config.update(config)

# log des r√©sultats
# log des r√©sultats
# Accessing the report with the correct key '-1.0' instead of '-1'
wandb.log({"precision_class_0": report["-1.0"]["precision"],
           "recall_class_0": report["-1.0"]["recall"],
           "f1_class_0": report["-1.0"]["f1-score"]})
wandb.log({"precision_class_1": report["1.0"]["precision"],
           "recall_class_1": report["1.0"]["recall"],
           "f1_class_1": report["1.0"]["f1-score"]})

#archiver le mod√®le dans le run
loaded_model = 'random_forest_model.pkl'
joblib.dump(random_forest, loaded_model)
wandb.save(loaded_model)
# archiver le mod√®le dans le model registry de wandb
artifact = wandb.Artifact("arbrededecision", type="model")
artifact.add_file(loaded_model)
wandb.log_artifact(artifact)

wandb.finish()